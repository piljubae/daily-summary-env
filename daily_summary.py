#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ActivityWatch ì¼ì¼ ìš”ì•½ ìƒì„±ê¸°
ë§¤ì¼ ì–´ì œì˜ í™œë™ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ ìš”ì•½ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

ë¼ì´ì„ ìŠ¤: MPL-2.0
"""

import json
import glob
import os
import re
import argparse
import requests
import sys
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict
from urllib.parse import urlparse

# ============================================================================
# ì„¤ì • ì„¹ì…˜ (ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥)
# ============================================================================

def load_env():
    """ë¡œì»¬ .env íŒŒì¼ì´ ìˆìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ë¡œ ë¡œë“œ (GitHubì—ëŠ” ì˜¬ë¼ê°€ì§€ ì•ŠìŒ)"""
    env_path = Path(__file__).parent / ".env"
    if env_path.exists():
        loaded_count = 0
        with open(env_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    key = key.strip()
                    value = value.strip()
                    # ë”°ì˜´í‘œ ì œê±° (ì˜ˆ: "value" -> value)
                    if (value.startswith('"') and value.endswith('"')) or (value.startswith("'") and value.endswith("'")):
                        value = value[1:-1]
                    os.environ[key] = value
                    loaded_count += 1
        if loaded_count > 0:
            print(f"âœ… .env íŒŒì¼ì—ì„œ {loaded_count}ê°œì˜ ì„¤ì •ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return True
    return False

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ ì¦‰ì‹œ .env ë¡œë“œ
load_env()

CONFIG = {
    # ActivityWatch API ì—°ê²° ì •ë³´
    "api_host": "127.0.0.1",
    "api_port": 5600,

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: í™ˆ ë””ë ‰í† ë¦¬/daily-summaries/)
    "output_dir": str(Path.home() / "daily-summaries"),

    # ìµœì†Œ í‘œì‹œ ê¸°ê°„ (ì´ˆ ë‹¨ìœ„, ì´ë³´ë‹¤ ì‘ì€ í™œë™ì€ ì œì™¸)
    "min_duration_seconds": 10,

    # ìƒìœ„ í‘œì‹œ ê°œìˆ˜
    "top_apps_count": 15,
    "top_urls_count": 10,

    # ìƒì‚°ì„± ì‹œê°„ëŒ€ ì •ì˜ (ì‹œê°„ ë²”ìœ„, 24ì‹œê°„ í˜•ì‹)
    "productive_hours": [(9, 12), (14, 18)],  # 9-12ì‹œ, 14-18ì‹œ

    # Cowork ì„¸ì…˜ ë¡œê·¸ ë””ë ‰í† ë¦¬
    # macOS: ~/Library/Application Support/Claude/projects/
    # Linux: ~/.config/Claude/projects/
    # ë¹ˆ ë¬¸ìì—´ì´ë©´ Cowork ìš”ì•½ ìƒëµ
    "cowork_log_dir": str(Path.home() / "Library" / "Application Support" / "Claude" / "projects"),

    # Slack Incoming Webhook URL
    # Slack ì•± â†’ Incoming Webhooks ì—ì„œ ë°œê¸‰
    # ë¹ˆ ë¬¸ìì—´ì´ë©´ Slack ì „ì†¡ ìƒëµ (ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë§Œ ìƒì„±)
    "slack_webhook_url": "",  # í™˜ê²½ë³€ìˆ˜ SLACK_WEBHOOK_URL ë˜ëŠ” ì§ì ‘ ì…ë ¥
    
    # Gemini API Key (AI ìš”ì•½ìš©)
    # ë¹ˆ ë¬¸ìì—´ì´ë©´ AI ìš”ì•½ ìƒëµ
    "gemini_api_key": "",  # í™˜ê²½ë³€ìˆ˜ GEMINI_API_KEY ë˜ëŠ” ì§ì ‘ ì…ë ¥
}

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def format_seconds(seconds):
    """ì´ˆë¥¼ ì‹œê°„:ë¶„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if seconds < 60:
        return f"{int(seconds)}ì´ˆ"

    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)

    if hours > 0:
        return f"{hours}ì‹œê°„ {minutes}ë¶„"
    return f"{minutes}ë¶„"


def get_api_url(endpoint):
    """ActivityWatch API URL ìƒì„±"""
    return f"http://{CONFIG['api_host']}:{CONFIG['api_port']}/api/0/{endpoint}"


def get_bucket_id(bucket_type):
    """ì§€ì •ëœ íƒ€ì…ì˜ ì²« ë²ˆì§¸ ë²„í‚· ID ë°˜í™˜"""
    try:
        # ë²„í‚· ëª©ë¡ ì¡°íšŒ (trailing slash í•„ìˆ˜)
        url = get_api_url("buckets/")
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        
        buckets = response.json()
        for bucket_id, bucket in buckets.items():
            if bucket.get("type") == bucket_type:
                return bucket_id
                
    except Exception as e:
        print(f"âš ï¸ ë²„í‚· ì¡°íšŒ ì‹¤íŒ¨ ({bucket_type}): {e}", file=sys.stderr)
    
    return None


def get_bucket_ids(bucket_type):
    """ì§€ì •ëœ íƒ€ì…ì˜ ëª¨ë“  ë²„í‚· ID ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    try:
        url = get_api_url("buckets/")
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        
        buckets = response.json()
        matching_buckets = []
        for bucket_id, bucket in buckets.items():
            if bucket.get("type") == bucket_type:
                matching_buckets.append(bucket_id)
        
        return matching_buckets
                
    except Exception as e:
        print(f"âš ï¸ ë²„í‚· ì¡°íšŒ ì‹¤íŒ¨ ({bucket_type}): {e}", file=sys.stderr)
    
    return []


def get_daterange(target_date):
    """ì§€ì •ëœ ë‚ ì§œì˜ ì‹œì‘ê³¼ ë ì‹œê°„ì„ ISO í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
    start = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
    end = start + timedelta(days=1)

    return start.isoformat(), end.isoformat()



def fetch_window_events(start_iso, end_iso):
    """ìœˆë„ìš° í™œë™ ë°ì´í„° ì¡°íšŒ

    Returns:
        dict: {'ì•±ì´ë¦„': ì´_ì‹œê°„_ì´ˆ} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
    """

    bucket_id = get_bucket_id("currentwindow")
    if not bucket_id:
        print("âš ï¸ ìœˆë„ìš° í™œë™ ë²„í‚·ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr)
        return {}

    try:
        url = get_api_url(f"buckets/{bucket_id}/events")
        params = {
            "start": start_iso,
            "end": end_iso,
            "limit": -1,
        }

        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()

        events = response.json()
        app_durations = defaultdict(float)

        for event in events:
            if "data" in event and "app" in event["data"]:
                app_name = event["data"]["app"]
                duration = event.get("duration", 0)

                if duration > CONFIG["min_duration_seconds"]:
                    app_durations[app_name] += duration

        return dict(app_durations)

    except Exception as e:
        print(f"âš ï¸ ìœˆë„ìš° í™œë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}", file=sys.stderr)
        return {}


def fetch_web_events(start_iso, end_iso):
    """ì›¹ ë¸Œë¼ìš°ì§• í™œë™ ë°ì´í„° ì¡°íšŒ

    Returns:
        dict: {'ë„ë©”ì¸': ì´_ì‹œê°„_ì´ˆ} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
    """

    bucket_ids = get_bucket_ids("web.tab.current")
    if not bucket_ids:
        print("âš ï¸ ì›¹ í™œë™ ë²„í‚·ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr)
        return {}, []

    domain_durations = defaultdict(float)
    url_details = []

    # ëª¨ë“  ì›¹ ë²„í‚·ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
    for bucket_id in bucket_ids:
        try:
            url = get_api_url(f"buckets/{bucket_id}/events")
            params = {
                "start": start_iso,
                "end": end_iso,
                "limit": -1,
            }

            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()

            events = response.json()

            for event in events:
                if "data" in event:
                    data = event["data"]
                    event_url = data.get("url", "")
                    duration = event.get("duration", 0)

                    if duration > CONFIG["min_duration_seconds"] and event_url:
                        try:
                            domain = urlparse(event_url).netloc or event_url
                            domain_durations[domain] += duration
                            url_details.append({
                                "url": event_url,
                                "domain": domain,
                                "duration": duration,
                                "title": data.get("title", "")
                            })
                        except Exception:
                            pass

        except Exception as e:
            print(f"âš ï¸ ì›¹ í™œë™ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ (ë²„í‚·: {bucket_id}): {e}", file=sys.stderr)
            continue

    return dict(domain_durations), url_details


def fetch_cowork_sessions(target_date):
    """Cowork ì„¸ì…˜ ë¡œê·¸ì—ì„œ í•´ë‹¹ ë‚ ì§œì˜ ëŒ€í™”ë¥¼ ì‘ì—… ë‹¨ìœ„ë¡œ ì¶”ì¶œ

    ì‚¬ìš©ì ë©”ì‹œì§€ (ì‘ì—… ì˜ë„)ì™€ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ (ê²°ê³¼ë¬¼, ì°¸ê³  ë¦¬ì†ŒìŠ¤)ì„
    í•¨ê»˜ ì½ì–´ì„œ "ë¬´ì—‡ì„ í•˜ë ¤ í–ˆê³ , ë¬´ì—‡ì„ ì°¸ê³ í–ˆëŠ”ì§€" ë‹¨ìœ„ë¡œ ë¬¶ìŠµë‹ˆë‹¤.

    Returns:
        list: [{"intent": str, "result": str, "urls": list}] í˜•ì‹
    """
    log_dir = CONFIG.get("cowork_log_dir", "")
    if not log_dir or not os.path.isdir(log_dir):
        return []

    raw_messages = []  # ì‹œê°„ìˆœ ì „ì²´ ë©”ì‹œì§€ ìˆ˜ì§‘

    try:
        jsonl_files = glob.glob(os.path.join(log_dir, "**", "*.jsonl"), recursive=True)

        for filepath in jsonl_files:
            mod_time = datetime.fromtimestamp(os.path.getmtime(filepath)).date()
            # Original logic for processing jsonl files
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            entry = json.loads(line)
                        except json.JSONDecodeError:
                            continue

                        ts_str = entry.get("timestamp", "")
                        if not ts_str:
                            continue
                        try:
                            ts = datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
                            entry_date = ts.date()
                        except (ValueError, TypeError):
                            continue
                        if entry_date != target_date.date():
                            continue

                        if entry.get("isMeta"):
                            continue

                        msg = entry.get("message", {})
                        role = msg.get("role", "")
                        if role not in ("user", "assistant"):
                            continue

                        content = msg.get("content", "")
                        if isinstance(content, list):
                            text_parts = []
                            for part in content:
                                if isinstance(part, dict) and part.get("type") == "text":
                                    text_parts.append(part.get("text", ""))
                            content = " ".join(text_parts)

                        if not content or len(content.strip()) < 2:
                            continue

                        raw_messages.append({
                            "timestamp": ts.strftime("%H:%M"),
                            "role": role,
                            "content": content.strip(),
                        })

            except (IOError, PermissionError):
                continue

    except Exception as e:
        print(f"âš ï¸ Cowork ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {e}", file=sys.stderr)

    raw_messages.sort(key=lambda x: x["timestamp"])

    # ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ì¤€ìœ¼ë¡œ ì‘ì—… ë‹¨ìœ„ ë¬¶ê¸°
    # ê° ì‚¬ìš©ì ë©”ì‹œì§€ = ì‘ì—… ì˜ë„, ì´í›„ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì—ì„œ ê²°ê³¼ì™€ URL ì¶”ì¶œ
    tasks = []
    i = 0
    while i < len(raw_messages):
        if raw_messages[i]["role"] == "user":
            intent = raw_messages[i]["content"]
            if len(intent) > 100:
                intent = intent[:100] + "..."

            # ë’¤ë”°ë¥´ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìˆ˜ì§‘
            result_text = ""
            urls = []
            j = i + 1
            while j < len(raw_messages) and raw_messages[j]["role"] == "assistant":
                resp = raw_messages[j]["content"]
                # ì‘ë‹µì—ì„œ URL ì¶”ì¶œ
                found_urls = re.findall(r'https?://[^\s\)\]>"]+', resp)
                for u in found_urls:
                    domain = urlparse(u).netloc
                    if domain and domain not in [urlparse(x).netloc for x in urls]:
                        urls.append(u)
                # ì²« ì‘ë‹µì˜ ì²« ë¬¸ì¥ì„ ê²°ê³¼ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
                if not result_text:
                    first_line = resp.split("\n")[0].strip()
                    # ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±°
                    first_line = re.sub(r'^[#*>\-\s]+', '', first_line)
                    if len(first_line) > 80:
                        first_line = first_line[:80] + "..."
                    result_text = first_line
                j += 1

            tasks.append({
                "intent": intent,
                "result": result_text,
                "urls": urls[:3],  # ë„ë©”ì¸ ê¸°ì¤€ ìƒìœ„ 3ê°œ
            })
            i = j
        else:
            i += 1

    # ê°™ì€ ì˜ë„ì˜ ì‘ì—…ì„ ë¬¶ê¸° (ì—°ì†ëœ ì§§ì€ í›„ì† ìš”ì²­ í†µí•©)
    merged_tasks = []
    for task in tasks:
        # ë„ˆë¬´ ì§§ì€ ë©”ì‹œì§€(2ê¸€ì ì´í•˜)ëŠ” ì´ì „ ì‘ì—…ì˜ í›„ì†ìœ¼ë¡œ ê°„ì£¼
        if merged_tasks and len(task["intent"]) <= 10 and not task["urls"]:
            merged_tasks[-1]["urls"].extend(task["urls"])
        else:
            merged_tasks.append(task)

    return merged_tasks


def fetch_claude_context(target_date):
    """ì§€ì •ëœ ë‚ ì§œì˜ Claude ì„¸ì…˜ í™œë™(ì˜ë„ ë° ì½”ë“œ ë³€ê²½)ì„ ì¶”ì¶œ"""
    sessions_dir = Path.home() / "Library/Application Support/Claude/local-agent-mode-sessions"
    context_data = []

    if not sessions_dir.exists():
        return context_data

    # Find relevant sessions
    for json_path in sessions_dir.rglob("*.json"):
        try:
            if "todos" in str(json_path):
                 continue

            with open(json_path, 'r', encoding='utf-8') as f:
                try:
                    metadata = json.load(f)
                except json.JSONDecodeError:
                    continue

            last_activity = metadata.get('lastActivityAt')
            if not last_activity:
                continue

            activity_date = datetime.fromtimestamp(last_activity / 1000).date()
            if activity_date != target_date.date():
                continue

            session_id = metadata.get('sessionId')
            title = metadata.get('title', 'Untitled Session')
            audit_path = json_path.parent / json_path.stem / "audit.jsonl"
            
            if not audit_path.exists():
                continue

            # Stats trackers
            start_time = None
            end_time = None
            interaction_count = 0
            
            # Goal & Actions
            initial_goal = "No user input found"
            files_created = set()
            files_modified = set()
            full_messages = []  # For detailed reporting
            
            # Helper to extract file path from tool input
            def get_path(tool_input):
                 return tool_input.get('file_path') or tool_input.get('TargetFile') or tool_input.get('path') or tool_input.get('AbsolutePath')

            with open(audit_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        entry = json.loads(line)
                        ts_str = entry.get('timestamp') # Assuming timestamp exists in audit logs or we use line order
                        
                        # Fallback for timestamp if not in audit log root, check message? 
                        # Actually audit logs usually have top-level timestamp or we just estimate from events.
                        # For simplicity, let's just use the order for first/last if timestamp isn't easy.
                        # If 'createdAt' in metadata is start, we might skip start_time here.
                        # But let's try to find timestamps in entry if available, else ignored for now.
                        
                        entry_type = entry.get('type')
                        
                        if entry_type == "user":
                            interaction_count += 1
                            msg = entry.get('message', {})
                            content = msg.get('content', '')
                            text_content = ""
                            
                            if isinstance(content, str):
                                text_content = content
                            elif isinstance(content, list):
                                for item in content:
                                    if item.get('type') == 'text':
                                        text_content = item.get('text', '')
                                        break
                            
                            if text_content:
                                if interaction_count == 1:
                                    initial_goal = text_content[:200] + "..." if len(text_content) > 200 else text_content
                                # Collect all messages for detailed list
                                full_messages.append(text_content.strip())

                        if entry_type == "assistant":
                            msg = entry.get('message', {})
                            content = msg.get('content', [])
                            if isinstance(content, list):
                                for item in content:
                                    if item.get('type') == 'tool_use':
                                        tool_name = item.get('name')
                                        tool_input = item.get('input', {})
                                        
                                        fpath = get_path(tool_input)
                                        if fpath:
                                            fname = Path(fpath).name
                                            if tool_name in ['write_to_file']:
                                                files_created.add(fname)
                                            elif tool_name in ['Edit', 'Replace', 'replace_file_content', 'multi_replace_file_content']:
                                                files_modified.add(fname)

                    except json.JSONDecodeError:
                        continue
            
            # Duration calculation (approximate using metadata if audit timestamps missing)
            # metadata has 'createdAt' and 'lastActivityAt'
            created_at = metadata.get('createdAt', 0)
            last_activity_at = metadata.get('lastActivityAt', 0)
            duration_minutes = 0
            if created_at and last_activity_at:
                duration_minutes = int((last_activity_at - created_at) / 1000 / 60)

            context_data.append({
                'session_id': session_id,
                'title': title,
                'duration_min': duration_minutes,
                'interaction_count': interaction_count,
                'goal': initial_goal,
                'files_created': sorted(list(files_created)),
                'files_modified': sorted(list(files_modified)),
                'full_messages': full_messages  # Add full conversation list
            })

        except Exception:
            continue

    return context_data


def categorize_apps(app_durations):
    """ì•±ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜

    Args:
        app_durations: {'ì•±ì´ë¦„': ì‹œê°„_ì´ˆ} ë”•ì…”ë„ˆë¦¬

    Returns:
        dict: ì¹´í…Œê³ ë¦¬ë³„ ì•± ì •ë³´
    """
    categories = {
        "ê°œë°œ": {
            "keywords": ["vscode", "visual studio", "android studio", "terminal", "iterm",
                        "cmd", "powershell", "intellij", "pycharm", "sublime"],
            "apps": defaultdict(float)
        },
        "ë¸Œë¼ìš°ì €": {
            "keywords": ["chrome", "firefox", "safari", "edge", "brave"],
            "apps": defaultdict(float)
        },
        "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜": {
            "keywords": ["slack", "teams", "discord", "telegram", "zoom", "mail"],
            "apps": defaultdict(float)
        },
    }

    categorized = {cat: {"apps": {}} for cat in categories}
    uncategorized = {}

    for app_name, duration in app_durations.items():
        found = False
        for category, info in categories.items():
            for keyword in info["keywords"]:
                if keyword.lower() in app_name.lower():
                    categorized[category]["apps"][app_name] = duration
                    found = True
                    break
            if found:
                break

        if not found:
            uncategorized[app_name] = duration

    # ì¹´í…Œê³ ë¦¬ë³„ ì†Œê³„ ê³„ì‚°
    for category in categories:
        total = sum(categorized[category]["apps"].values())
        categorized[category]["total"] = total

    categorized["ê¸°íƒ€"] = {
        "apps": uncategorized,
        "total": sum(uncategorized.values())
    }

    return categorized


def calculate_active_time(app_durations, domain_durations):
    """ì „ì²´ í™œë™ ì‹œê°„ ê³„ì‚°

    Returns:
        tuple: (ì´_í™œë™_ì‹œê°„_ì´ˆ, ì‹œê°„ëŒ€ë³„_í™œë™_ì‹œê°„)
    """
    total = sum(app_durations.values())

    # ì‹œê°„ëŒ€ë³„ í™œë™ ì‹œê°„ ê³„ì‚° (ê°„ë‹¨í•œ ì¶”ì •)
    hourly_activity = defaultdict(float)
    if app_durations:
        avg_per_app = total / len(app_durations)
        for i, (app, duration) in enumerate(sorted(app_durations.items(), key=lambda x: x[1], reverse=True)):
            hour = i % 24
            hourly_activity[hour] += duration

    return total, dict(hourly_activity)


def generate_productivity_summary(hourly_activity):
    """ìƒì‚°ì„± ì‹œê°„ëŒ€ ìš”ì•½ ìƒì„±"""
    productive_time = 0

    for start_hour, end_hour in CONFIG["productive_hours"]:
        for hour in range(start_hour, end_hour):
            productive_time += hourly_activity.get(hour, 0)

    return productive_time


def generate_one_liner(app_durations, domain_durations, total_time):
    """í•œì¤„ ìš”ì•½ ìƒì„± (AI ì—†ì´ ê·œì¹™ ê¸°ë°˜)"""
    if not app_durations:
        return "ì˜¤ëŠ˜ì€ ì»´í“¨í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    top_app = sorted(app_durations.items(), key=lambda x: x[1], reverse=True)[0]
    app_name = top_app[0]
    duration = format_seconds(top_app[1])

    if "chrome" in app_name.lower() or "firefox" in app_name.lower() or "safari" in app_name.lower():
        return f"ì£¼ë¡œ ì›¹ ë¸Œë¼ìš°ì§•ì— {duration} ì‹œê°„ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
    elif "code" in app_name.lower() or "studio" in app_name.lower():
        return f"ê°œë°œ ì‘ì—…ì— {duration}ì„ ì§‘ì¤‘í–ˆìŠµë‹ˆë‹¤."
    elif "slack" in app_name.lower() or "teams" in app_name.lower():
        return f"ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ë„êµ¬ì— {duration}ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
    else:
        return f"{app_name}ì— ê°€ì¥ ë§ì€ ì‹œê°„({duration})ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."


def create_markdown_report(app_durations, domain_durations, url_details, target_date):
    """5ì¤„ ì´ë‚´ í•µì‹¬ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±

    Returns:
        str: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ê°„ê²°í•œ ë³´ê³ ì„œ
    """
    total_time, _ = calculate_active_time(app_durations, domain_durations)

    report = f"# {target_date.strftime('%m/%d')} ì¼ì¼ ìš”ì•½\n\n"

    # 1ì¤„: ì´ í™œë™ ì‹œê°„ + ê°€ì¥ ë§ì´ ì“´ ì•± ìƒìœ„ 3ê°œ
    if app_durations:
        top_apps = sorted(app_durations.items(), key=lambda x: x[1], reverse=True)[:3]
        apps_str = ", ".join(f"{name} {format_seconds(dur)}" for name, dur in top_apps)
        report += f"**ğŸ’» {format_seconds(total_time)}** â€” {apps_str}\n\n"

    # 2ì¤„: ì£¼ìš” ë°©ë¬¸ ì‚¬ì´íŠ¸ + í•µì‹¬ í˜ì´ì§€ ì œëª©
    if domain_durations:
        top_domains = sorted(domain_durations.items(), key=lambda x: x[1], reverse=True)[:3]
        site_parts = []
        for rank, (domain, dur) in enumerate(top_domains, 1):
            # í•´ë‹¹ ë„ë©”ì¸ì—ì„œ ê°€ì¥ ì˜¤ë˜ ë³¸ í˜ì´ì§€ ì œëª© 1ê°œ
            domain_pages = [u for u in url_details if u["domain"] == domain and u.get("title")]
            page_durations = defaultdict(float)
            for p in domain_pages:
                title = p["title"].strip()
                if title:
                    page_durations[title] += p["duration"]
            if page_durations:
                top_page = sorted(page_durations.items(), key=lambda x: x[1], reverse=True)[0][0]
                # í˜ì´ì§€ ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                if len(top_page) > 40:
                    top_page = top_page[:40] + "..."
                site_parts.append(f"{rank}. {domain} ({top_page})")
            else:
                site_parts.append(f"{rank}. {domain}")
        report += f"**ğŸŒ ì‚¬ì´íŠ¸** â€” {' / '.join(site_parts)}\n\n"

    # 3~4ì¤„: Cowork ì‘ì—… ìš”ì•½ (ì˜ë„ + ê²°ê³¼ + ì°¸ê³  ë¦¬ì†ŒìŠ¤)
    cowork_tasks = fetch_cowork_sessions(target_date)
    if cowork_tasks:
        report += f"**ğŸ¤– Cowork** ({len(cowork_tasks)}ê±´)\n"
        for task in cowork_tasks[:7]:
            line = f"- {task['intent']}"
            if task["result"]:
                line += f" â€” {task['result']}"
            report += line + "\n"
            # ì°¸ê³ í•œ URLì´ ìˆìœ¼ë©´ ë„ë©”ì¸ë§Œ ê°„ê²°í•˜ê²Œ í‘œì‹œ
            if task["urls"]:
                domains = [urlparse(u).netloc for u in task["urls"]]
                report += f"  ğŸ“ {', '.join(domains)}\n"
        if len(cowork_tasks) > 7:
            report += f"- ...ì™¸ {len(cowork_tasks) - 7}ê±´\n"
        report += "\n"

    # ğŸ¤– Claude í™œë™ (Local Agent)
    claude_context = fetch_claude_context(target_date)
    if claude_context:
        report += f"**ğŸ¤– Claude í™œë™**\n"
        for session in claude_context:
            title = session.get('title', 'ì„¸ì…˜')
            duration = session.get('duration_min', 0)
            count = session.get('interaction_count', 0)
            
            report += f"### ğŸ“‚ {title}\n"
            report += f"> â±ï¸ **{duration}ë¶„** ë™ì•ˆ **{count}ë²ˆ**ì˜ ìƒí˜¸ì‘ìš©\n\n"
            
            report += f"**ğŸ¯ ì‘ì—… ëª©í‘œ**\n"
            report += f"{session['goal']}\n\n"
            
            has_changes = False
            if session['files_created']:
                report += f"- ğŸ†• **ìƒì„±ëœ íŒŒì¼**: {', '.join(session['files_created'])}\n"
                has_changes = True
            if session['files_modified']:
                report += f"- ğŸ“ **ìˆ˜ì •ëœ íŒŒì¼**: {', '.join(session['files_modified'])}\n"
                has_changes = True
            
            if not has_changes:
                report += "- âš ï¸ íŒŒì¼ ë³€ê²½ ì‚¬í•­ ì—†ìŒ\n"
                
            report += "\n"

    # ìƒì„¸ í™œë™ ëª©ë¡ (Detailed Lists)
    report += "---\n\n"
    report += "## ğŸ“‹ ìƒì„¸ í™œë™ ëª©ë¡\n\n"
    
    # Claude ì „ì²´ ëŒ€í™” ëª©ë¡
    if claude_context:
        for session in claude_context:
            title = session.get('title', 'ì„¸ì…˜')
            full_messages = session.get('full_messages', [])
            
            if full_messages:
                report += f"### ğŸ’¬ Claude: {title}\n"
                for idx, msg in enumerate(full_messages, 1):
                    # Truncate very long messages
                    display_msg = msg[:150] + "..." if len(msg) > 150 else msg
                    display_msg = display_msg.replace("\n", " ")
                    report += f"{idx}. {display_msg}\n"
                report += "\n"
    
    # ì›¹ì‚¬ì´íŠ¸ íƒ€ì´í‹€ ëª©ë¡
    if url_details:
        report += "### ğŸŒ ë°©ë¬¸í•œ ì›¹í˜ì´ì§€\n"
        # Collect unique titles with URLs
        unique_pages = {}
        for u in url_details:
            title = u.get("title", "").strip()
            url = u.get("url", "")
            if title and url and title not in unique_pages:
                unique_pages[title] = url
        
        for idx, (title, url) in enumerate(sorted(unique_pages.items()), 1):
            display_title = title[:100] + "..." if len(title) > 100 else title
            report += f"{idx}. [{display_title}]({url})\n"
        report += "\n"

    # 4ì¤„: í•œì¤„ ìš”ì•½
    one_liner = generate_one_liner(app_durations, domain_durations, total_time)
    report += f"> {one_liner}\n"

    return report


def send_to_slack(markdown_content):
    """Slack Incoming Webhookìœ¼ë¡œ ë³´ê³ ì„œ ì „ì†¡

    ë§ˆí¬ë‹¤ìš´ì„ Slack mrkdwn í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    webhook_url = CONFIG.get("slack_webhook_url", "")
    if not webhook_url:
        return False

    # ë§ˆí¬ë‹¤ìš´ â†’ Slack mrkdwn ë³€í™˜
    slack_text = markdown_content
    
    # [í…ìŠ¤íŠ¸](URL) -> <URL|í…ìŠ¤íŠ¸> ë³€í™˜ (Slack í˜•ì‹)
    slack_text = re.sub(r'\[([^\]]+)\]\(([^\)]+)\)', r'<\2|\1>', slack_text)
    
    slack_text = re.sub(r'^# (.+)$', r'*\1*', slack_text, flags=re.MULTILINE)       # h1 â†’ bold
    slack_text = re.sub(r'\*\*(.+?)\*\*', r'*\1*', slack_text)                       # **bold** â†’ *bold*
    slack_text = re.sub(r'^\- ', 'â€¢ ', slack_text, flags=re.MULTILINE)               # - â†’ â€¢
    slack_text = re.sub(r'^  ğŸ“', '    ğŸ“', slack_text, flags=re.MULTILINE)          # ë“¤ì—¬ì“°ê¸° ë³´ì •

    payload = {
        "text": slack_text,
        "unfurl_links": False,
    }

    try:
        response = requests.post(webhook_url, json=payload, timeout=10)
        response.raise_for_status()
        return True
    except Exception as e:
        print(f"âš ï¸ Slack ì „ì†¡ ì‹¤íŒ¨: {e}", file=sys.stderr)
        return False


def save_report(markdown_content, target_date):
    """ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    output_dir = Path(CONFIG["output_dir"])
    output_dir.mkdir(parents=True, exist_ok=True)

    filename = f"{target_date.date().isoformat()}-daily-summary.md"
    filepath = output_dir / filename

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(markdown_content)

    return filepath


def summarize_with_gemini(md_content, api_key):
    """Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ì¼ ìš”ì•½ì„ 5ê°€ì§€ í•µì‹¬ í¬ì¸íŠ¸ë¡œ ìš”ì•½"""
    if not api_key:
        return None
    
    try:
        import requests
        import json
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={api_key}"
        
        prompt = f"""ë‹¤ìŒì€ í•˜ë£¨ ë™ì•ˆì˜ í™œë™ ìš”ì•½ ë¦¬í¬íŠ¸ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ì½ê³  **5ê°€ì§€ í•µì‹¬ í™œë™**ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ê° í•­ëª©ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
2. ì›ë³¸ ë¦¬í¬íŠ¸ì— ìˆëŠ” ì¤‘ìš”í•œ ë§í¬ëŠ” ë°˜ë“œì‹œ í¬í•¨ (ë§ˆí¬ë‹¤ìš´ ë§í¬ í˜•ì‹ ìœ ì§€)
3. ì‹œê°„ ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨
4. ë²ˆí˜¸ ë§¤ê¸°ê¸° (1. 2. 3. 4. 5.)
5. í•œêµ­ì–´ë¡œ ì‘ì„±

ë¦¬í¬íŠ¸ ë‚´ìš©:
{md_content}

5ê°€ì§€ í•µì‹¬ í™œë™:"""

        payload = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }]
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
        response.raise_for_status()
        
        result = response.json()
        return result['candidates'][0]['content']['parts'][0]['text']
        
    except Exception as e:
        print(f"âš ï¸ Gemini API ìš”ì•½ ì‹¤íŒ¨: {e}", file=sys.stderr)
        return None
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ActivityWatch Daily Summary Generator")
    parser.add_argument("date", nargs="?", help="ìš”ì•½í•  ë‚ ì§œ (YYYYMMDD í˜•ì‹). ìƒëµ ì‹œ ì–´ì œ ë˜ëŠ” --today ì˜µì…˜ ì‚¬ìš©")
    parser.add_argument("--today", action="store_true", help="ì˜¤ëŠ˜ ë‚ ì§œì˜ ìš”ì•½ ìƒì„± (ê¸°ë³¸ê°’: ì–´ì œ)")
    args = parser.parse_args()

    # ë‚ ì§œ ì„¤ì •
    target_date = None

    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y%m%d")
            date_label = f"{args.date}"
        except ValueError:
            print("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYYMMDD í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì˜ˆ: 20260210)", file=sys.stderr)
            return 1
    elif args.today:
        target_date = datetime.now()
        date_label = "ì˜¤ëŠ˜"
    else:
        target_date = datetime.now() - timedelta(days=1)
        date_label = "ì–´ì œ"

    start_iso, end_iso = get_daterange(target_date)

    print(f"ğŸ”„ ActivityWatch {date_label}({target_date.strftime('%Y-%m-%d')}) ìš”ì•½ ìƒì„± ì¤‘...")
    print(f"ğŸ“ API ì—°ê²°: {CONFIG['api_host']}:{CONFIG['api_port']}")

    # ë°ì´í„° ì¡°íšŒ
    print("ğŸ“¥ í™œë™ ë°ì´í„° ì¡°íšŒ ì¤‘...")
    app_durations = fetch_window_events(start_iso, end_iso)
    domain_durations, url_details = fetch_web_events(start_iso, end_iso)

    if not app_durations and not domain_durations:
        print("âš ï¸ ì¡°íšŒëœ í™œë™ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   ActivityWatchê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return 1

    cowork_count = len(fetch_cowork_sessions(target_date))
    print(f"âœ… ì•± í™œë™ {len(app_durations)}ê°œ, ì›¹ í™œë™ {len(domain_durations)}ê°œ, Cowork ìš”ì²­ {cowork_count}ê±´ ì¡°íšŒë¨")

    # ë³´ê³ ì„œ ìƒì„±
    print("ğŸ“ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    markdown_content = create_markdown_report(app_durations, domain_durations, url_details, target_date)

    # íŒŒì¼ ì €ì¥
    print("ğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...")
    filepath = save_report(markdown_content, target_date)
    print(f"âœ… ë³´ê³ ì„œ ì €ì¥: {filepath}")

    # AI ìš”ì•½ ìƒì„± ë° ì¶”ê°€
    gemini_api_key = CONFIG.get("gemini_api_key") or os.environ.get("GEMINI_API_KEY", "")
    ai_summary = None
    
    if gemini_api_key:
        print("ğŸ¤– AI ìš”ì•½ ìƒì„± ì¤‘...")
        ai_summary = summarize_with_gemini(markdown_content, gemini_api_key)
        
        if ai_summary:
            print("âœ… AI ìš”ì•½ ìƒì„± ì™„ë£Œ!")
            # MD íŒŒì¼ì— AI ìš”ì•½ ì¶”ê°€
            with open(filepath, 'a', encoding='utf-8') as f:
                f.write(f"\n\n---\n\n## ğŸ¤– AI ìš”ì•½ (Gemini)\n\n{ai_summary}\n")
            print("âœ… AI ìš”ì•½ì„ MD íŒŒì¼ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤")
        else:
            print("âš ï¸ AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
    else:
        print("â„¹ï¸ Gemini API Key ë¯¸ì„¤ì • â€” AI ìš”ì•½ ìƒëµ")

    # Slack ì „ì†¡
    slack_webhook_url = CONFIG.get("slack_webhook_url") or os.environ.get("SLACK_WEBHOOK_URL", "")
    if slack_webhook_url:
        # CONFIG["slack_webhook_url"]ì´ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ ì—…ë°ì´íŠ¸
        CONFIG["slack_webhook_url"] = slack_webhook_url
        
        if ai_summary:
            # AI ìš”ì•½ë§Œ Slackìœ¼ë¡œ ì „ì†¡
            print("ğŸ“¤ AI ìš”ì•½ë§Œ Slackìœ¼ë¡œ ì „ì†¡ ì¤‘...")
            summary_message = f"*ğŸ“Š {target_date.strftime('%m/%d')} ì¼ì¼ ìš”ì•½ (AI ìƒì„±)*\n\n{ai_summary}\n\n---\n*ìƒì„¸ ë¦¬í¬íŠ¸*: `{filepath}`"
            if send_to_slack(summary_message):
                print("âœ… Slack ì „ì†¡ ì™„ë£Œ!")
            else:
                print("âš ï¸ Slack ì „ì†¡ ì‹¤íŒ¨")
        else:
            # AI ìš”ì•½ì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì•Œë¦¼ë§Œ ì „ì†¡
            print("ğŸ“¤ ìš”ì•½ ì•Œë¦¼ì„ Slackìœ¼ë¡œ ì „ì†¡ ì¤‘...")
            
            if gemini_api_key:
                # í‚¤ëŠ” ìˆëŠ”ë° ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í•œ ê²½ìš°
                reason = "Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ë¡œê·¸ í™•ì¸ í•„ìš”)"
            else:
                # í‚¤ ìì²´ê°€ ì—†ëŠ” ê²½ìš°
                reason = "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ AI ìš”ì•½ì€ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤."
                
            alert_message = f"âœ… *{target_date.strftime('%m/%d')}* ì¼ì¼ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n*ìœ„ì¹˜*: `{filepath}`\n({reason})"
            if send_to_slack(alert_message):
                print("âœ… Slack ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ!")
            else:
                print("âš ï¸ Slack ì „ì†¡ ì‹¤íŒ¨")
    else:
        print("â„¹ï¸ Slack Webhook ë¯¸ì„¤ì • â€” íŒŒì¼ë§Œ ì €ì¥ë¨")

    return 0


if __name__ == "__main__":
    sys.exit(main())

def fetch_antigravity_activity(target_date):
    """í•´ë‹¹ ë‚ ì§œì˜ Antigravity í™œë™ ì¶”ì¶œ (Git ì´ë ¥ ê¸°ë°˜)"""
    import subprocess
    
    start = target_date.replace(hour=0, minute=0, second=0)
    end = start + timedelta(days=1)
    since = start.strftime("%Y-%m-%d 00:00:00")
    until = end.strftime("%Y-%m-%d 23:59:59")
    
    files_modified = set()
    work_dirs = [Path.home() / "daily-summary-env"]
    
    for work_dir in work_dirs:
        if not work_dir.exists():
            continue
        try:
            result = subprocess.run(
                ["git", "log", f"--since={since}", f"--until={until}", "--name-only", "--pretty=format:"],
                cwd=str(work_dir), capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0 and result.stdout:
                for line in result.stdout.strip().split('\n'):
                    line = line.strip()
                    if line and ('/' in line or '.' in line):
                        files_modified.add(line)
        except Exception:
            continue
    
    return {'files_modified': sorted(list(files_modified))[:20]}


